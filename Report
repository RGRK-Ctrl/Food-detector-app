ğŸ² Food Detector App â€” Consolidated Project Report (Activities & Decisions)
â¸»
1) Goal & Scope
	â€¢	Build an end-to-end Food image classifier with a browser UI.
	â€¢	Core flows: Data Analysis â†’ Model Training â†’ Evaluation â†’ Predictions.
	â€¢	UX goals: simple upload/predict, transparent metrics, feedback capture, and a clean, modern interface.
â¸»
2) Data & Preparation
	â€¢	Dataset: Custom â€œFood Classification datasetâ€ organized by class folders.
	â€¢	Splits: Stratified 80/10/10 (train/val/test) using two-stage train_test_split to preserve class balance.
	â€¢	Persistent split: Saved to train_val_test_split.npz (paths + integer labels).
	â€¢	Per-split mirroring: Copied images into TRAIN_DIR, VAL_DIR, TEST_DIR folder trees by class for easy inspection.
	â€¢	Class labels: Stored in class_labels.json as an ordered list; used consistently across training, evaluation, and app.
Summaries & reporting
	â€¢	Each split directory has a summary file (e.g., train_summary.txt) with:
	â€¢	Creation timestamp
	â€¢	Per-class image counts
	â€¢	Total image count (fixed an early â€œdouble countingâ€ bug)
	â€¢	UI reads training summary (created_at, num_classes, total_images, class_counts) from train_summary.txt + class_labels.json.
â¸»
3) Data Loading & Preprocessing
	â€¢	Introduced Image_Sequence.py with:
	â€¢	ImageSequence (Keras Sequence) for robust, batched loading.
	â€¢	preprocess_image(img, target_size, augment=False) for consistent resizing â†’ efficientnet_v2.preprocess_input.
	â€¢	Optional deterministic augmentation during training only.
	â€¢	Key decision: Use preprocess_image everywhere (training, evaluation, app).
	â€¢	Fixed critical bug where manual preprocessing caused single-class collapse at inference.

Performance upgrades (new):
	â€¢	Shuffling added: ds.shuffle(8192) ensures better randomness.
	â€¢	AUTOTUNE + prefetching: num_parallel_calls=tf.data.AUTOTUNE, prefetch(AUTOTUNE) â†’ speeds up pipeline.
	â€¢	Non-deterministic execution: opts.experimental_deterministic=False for extra throughput.

â¸»
4) Model Architecture & Training Strategy
	â€¢	Backbone: EfficientNetV2 (classification head adapted to number of classes).
	â€¢	Loss: categorical_crossentropy; Optimizer: Adam (LR ~1e-4 initially).
	â€¢	Class weights: Supported for imbalance.

Callbacks
	â€¢	ModelCheckpoint (best val metric)
	â€¢	EarlyStopping
	â€¢	ReduceLROnPlateau
	â€¢	EpochTimeLogger (custom; logs epoch duration, sends Telegram updates with ETA, accuracy, loss).

History tracking
	â€¢	Encouraged saving history.history to JSON/CSV â†’ UI charts (accuracy/loss per epoch).
	â€¢	Telegram messages extended to include per-epoch metrics.

Training time insights (new)
	â€¢	Stage 1 (frozen base): ~20 min for 2 epochs on Apple M2.
	â€¢	Stage 2 (fine-tune): ~1h40m for 2 epochs.
	â€¢	Added live ETA per epoch in console + Telegram.
	â€¢	Projected: 15â€“20 epochs could take ~10â€“15 hours depending on hardware.

â¸»
5) Evaluation Pipeline
	â€¢	Generators: ImageSequence for val/test; no augmentation during inference.
	â€¢	Metrics:
	â€¢	Overall accuracy
	â€¢	Classification report (precision/recall/F1 per class)
	â€¢	Confusion matrix (PNG saved)
	â€¢	Detailed CSV: image_path, true_label, predicted_label, correctness, top-3 confidences

Bulk prediction metrics (new):
	â€¢	Extracted from testdata_inference_report.txt:
	â€¢	âœ… Accuracy
	â€¢	âœ… Precision (weighted)
	â€¢	âœ… Recall (weighted)
	â€¢	âœ… F1-score (weighted)
	â€¢	Displayed in UI card below Final Metrics Summary.

Debug tools
	â€¢	Class index alignment check (class_labels.json vs. model output shape).
	â€¢	â€œFirst N imagesâ€ smoke test for sanity checks.

Extra artifacts (planned):
	â€¢	Misclassification examples with top-k confusion.
	â€¢	Per-class accuracy tables for interpretability.

â¸»
6) Notifications (Telegram)
	â€¢	send_telegram.py integration sends:
	â€¢	Per-epoch metrics + ETA
	â€¢	Training/val loss & accuracy
	â€¢	Evaluation summary + accuracy
	â€¢	Predicted class distribution
	â€¢	Lightweight reports (no large uploads).

â¸»
7) Flask Application (Backend)
	â€¢	Endpoints:
	â€¢	/ â†’ Predictions (upload multiple files, batch inference, top-k results)
	â€¢	/feedback â†’ Saves user-selected correct label images for retraining
	â€¢	Inference:
	â€¢	All inputs run through preprocess_image (consistent with training).
	â€¢	Batch size = 1 (low latency).
	â€¢	Returns class scores + top-k.
	â€¢	Fixes:
	â€¢	Shape mismatch resolved (resize + expand_dims order).
	â€¢	Augmentations disabled at inference.

Logging & monitoring (new):
	â€¢	Captures image name, prediction time per request (ms).
	â€¢	Logs low-confidence predictions flagged below threshold.
	â€¢	Error handling with safe fallbacks.

â¸»
8) Frontend (UI/UX)
	â€¢	Structure: Jinja layout (base.html + per-page blocks).
	â€¢	Navigation Tabs:
	1.	Data Analysis â†’ stats, splits, charts
	2.	Model Training â†’ config, per-epoch charts, checkpoints, training summary
	3.	Evaluation â†’ metrics, confusion matrix, downloadable CSV
	4.	Predictions â†’ upload, results, feedback

Enhancements (new)
	â€¢	Right-side Model Summary & Parameter Summary cards added alongside Training Summary.
	â€¢	Scrollable model summary card with larger width/height (fixed truncation issue).
	â€¢	Stats panel improvements:
	â€¢	Images tested (session)
	â€¢	Incorrect count
	â€¢	Confidence threshold

Future polish:
	â€¢	Responsive design for mobile.
	â€¢	Pagination/gallery view for large uploads.
	â€¢	Highlighting low-confidence predictions with orange warning indicators.

â¸»
9) File/Directory Layout (Current)

Food_Detector_App/
â”œâ”€â”€ app.py
â”œâ”€â”€ Model_data/
â”‚   â”œâ”€â”€ Food detector model.keras
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â”œâ”€â”€ class_labels.json
â”‚   â”‚   â””â”€â”€ training_log.csv
â”‚   â”œâ”€â”€ Reports/
â”‚   â”‚   â”œâ”€â”€ model_summary.txt
â”‚   â”‚   â””â”€â”€ model_param_summary.json
â”‚   â”œâ”€â”€ Checkpoints/
â”‚   â””â”€â”€ train_val_test_split.npz
â”œâ”€â”€ Food images Training set/
â”‚   â””â”€â”€ train_summary.txt
â”œâ”€â”€ Food images Validation set/
â”œâ”€â”€ Food images Test set/
â”œâ”€â”€ Predictions/
â”‚   â””â”€â”€ testdata_inference_report.txt
â”œâ”€â”€ Feedback dataset/
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ base.html
â”‚   â”œâ”€â”€ model_training.html
â”‚   â”œâ”€â”€ evaluation.html
â”‚   â””â”€â”€ index.html
â””â”€â”€ static/
    â”œâ”€â”€ css/styles.css
    â”œâ”€â”€ js/app.js
    â””â”€â”€ images/

â¸»
10) Debugging Highlights & Fixes
	â€¢	âŒ Single-class prediction issue â†’ Fixed via consistent preprocessing.
	â€¢	âŒ Shape mismatch errors â†’ Fixed image resize/expand order.
	â€¢	âŒ Double counting bug in summaries â†’ Fixed parser.
	â€¢	âŒ Workers param in .fit() â†’ Deprecated in TF 2.17; removed.
	â€¢	âœ… Epoch time logging & ETA now integrated.
	â€¢	âœ… UI card layout fixes â†’ wider, scrollable model summary.

â¸»
11) Deployment Planning
	â€¢	Target: Render (Flask deploy with Procfile + requirements).
	â€¢	Alternatives: Railway, Hugging Face Spaces, Fly.io.
	â€¢	Constraints: model size, memory, cold starts.

Security considerations (new):
	â€¢	Validate file uploads (only images).
	â€¢	Temporary storage only; images deleted after prediction.
	â€¢	No personal data retained.

â¸»
12) Performance & Dev Environment
	â€¢	Running on Apple M2 MacBook.
	â€¢	Using tensorflow-macos + tensorflow-metal acceleration.
	â€¢	Batch size = 16 (balanced speed/memory).
	â€¢	Training durations monitored; Stage 2 epochs are much longer than Stage 1.
	â€¢	Shuffle + AUTOTUNE improved throughput.

â¸»
13) Planned Enhancements (Backlog)
	â€¢	Complete landing page (hero, CTA).
	â€¢	Charts: Accuracy/Loss over epochs (Chart.js or static).
	â€¢	Export confusion matrix + CSV download.
	â€¢	Feedback retraining pipeline.
	â€¢	Model versioning & metadata in UI.
	â€¢	Public demo deployment.
	â€¢	CI/CD with automated tests (planned).
	â€¢	Drift detection to monitor post-deployment accuracy.

â¸»
14) Key Takeaways
	â€¢	Consistency of preprocessing is the #1 priority.
	â€¢	Artifact persistence (splits, labels, summaries) keeps everything aligned.
	â€¢	Epoch logging + ETA improves training monitoring.
	â€¢	UI transparency builds trust (bulk metrics, training stats, scrollable panels).
	â€¢	Performance trade-offs: shuffle/autotune â†’ faster, but less reproducible.
	â€¢	Security, monitoring, and retraining pipelines will be critical for scaling.

â¸»

âœ… Thatâ€™s the enriched master report including security, logging, retraining pipeline planning, UI polish, and monitoring.
